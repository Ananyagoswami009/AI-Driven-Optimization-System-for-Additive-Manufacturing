# -*- coding: utf-8 -*-
"""AM

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g882lDJUKLVA64qDf_PbMM3vtkFFNlbK
"""

import pandas as pd

# Load the dataset
data = pd.read_csv('/content/data.csv')

# Explore the dataset
print(data.head())
print(data.info())
print(data.describe())

from sklearn.preprocessing import LabelEncoder

# Encode infill_pattern and material
data['infill_pattern'] = LabelEncoder().fit_transform(data['infill_pattern'])
data['material'] = LabelEncoder().fit_transform(data['material'])

from sklearn.preprocessing import StandardScaler

# Select numerical features for scaling
numerical_cols = ['layer_height', 'wall_thickness', 'infill_density', 'nozzle_temperature',
                  'bed_temperature', 'print_speed', 'fan_speed']
scaler = StandardScaler()
data[numerical_cols] = scaler.fit_transform(data[numerical_cols])

X = data.drop(['roughness'], axis=1)
y = data['roughness']

X = data.drop(['tension_strenght', 'elongation'], axis=1)
y_tension = data['tension_strenght']
y_elongation = data['elongation']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("MSE:", mean_squared_error(y_test, y_pred))
print("R2 Score:", r2_score(y_test, y_pred))

import matplotlib.pyplot as plt

feature_importances = model.feature_importances_
plt.barh(X.columns, feature_importances)
plt.xlabel('Feature Importance')
plt.ylabel('Features')
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Assuming 'model' is the trained Random Forest model and 'X' contains the feature columns
feature_importances = model.feature_importances_
features = X.columns

# Sort features by importance for better visualization
sorted_idx = np.argsort(feature_importances)
sorted_features = [features[i] for i in sorted_idx]
sorted_importances = feature_importances[sorted_idx]

# Create the enhanced bar plot
plt.figure(figsize=(10, 8))
bars = plt.barh(sorted_features, sorted_importances, color='skyblue')

# Add data labels
for bar in bars:
    plt.text(bar.get_width() + 0.005, bar.get_y() + bar.get_height() / 2,
             f'{bar.get_width():.2f}', va='center', fontsize=10, color='black')

# Add grid and labels
plt.grid(axis='x', alpha=0.6)
plt.xlabel('Feature Importance', fontsize=14)
plt.ylabel('Features', fontsize=14)
plt.title('Feature Importance for Predicting Roughness', fontsize=16)

# Adjust tick parameters
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)

# Display the plot
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Assuming 'model' is the trained Random Forest model and 'X' contains the feature columns
feature_importances = model.feature_importances_
features = X.columns

# Create the bar plot
plt.figure(figsize=(10, 6))
plt.barh(features, feature_importances, color='skyblue')
plt.xlabel('Feature Importance')
plt.ylabel('Features')
plt.title('Feature Importance for Predicting Roughness')
plt.tight_layout()
plt.show()

# Ensure y_test and y_pred are numpy arrays
y_test = np.array(y_test)
y_pred = np.array(y_pred)

# Scatter plot with data labels
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.7, color='blue', label='Data Points')

# Add data labels
for i in range(len(y_test)):
    plt.text(y_test[i], y_pred[i], f'({y_test[i]:.1f}, {y_pred[i]:.1f})',
             fontsize=8, ha='right', va='bottom', color='black', alpha=0.8)

# Add perfect prediction line
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2, label='Perfect Prediction Line')

# Add labels, title, and legend
plt.xlabel('Actual Roughness')
plt.ylabel('Predicted Roughness')
plt.title('Predicted vs Actual Roughness')
plt.legend()
plt.tight_layout()

# Display the plot
plt.show()

import matplotlib.pyplot as plt

# Scatter plot for predicted vs actual roughness
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.7, color='blue', label='Data Points')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2, label='Perfect Prediction Line')  # Perfect prediction line

# Add data labels to each point
for i in range(len(y_test)):
    plt.text(y_test[i], y_pred[i], f'({y_test[i]:.1f}, {y_pred[i]:.1f})', fontsize=8, ha='right', va='bottom', color='black', alpha=0.8)

# Adding labels, title, and legend
plt.xlabel('Actual Roughness')
plt.ylabel('Predicted Roughness')
plt.title('Predicted vs Actual Roughness')
plt.legend()
plt.tight_layout()

# Display the plot
plt.show()

import seaborn as sns

# Combine features and target into one DataFrame for visualization
data['roughness'] = y
sns.pairplot(data[['layer_height', 'wall_thickness', 'infill_density', 'roughness']], diag_kind='kde', kind='scatter')
plt.suptitle('Feature Relationships with Roughness', y=1.02)
plt.show()

# Box plot for material vs roughness
plt.figure(figsize=(8, 6))
sns.boxplot(x='material', y='roughness', data=data)
plt.xlabel('Material')
plt.ylabel('Roughness')
plt.title('Roughness by Material')
plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 6))
residuals = y_test - y_pred
plt.scatter(y_pred, residuals, alpha=0.7, color='purple')
plt.axhline(y=0, color='red', linestyle='--', linewidth=2)
plt.xlabel('Predicted Roughness')
plt.ylabel('Residuals (Actual - Predicted)')
plt.title('Residual Plot for Model Predictions')
plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 6))
sns.boxplot(x='material', y='roughness', data=data)
plt.xlabel('Material')
plt.ylabel('Roughness')
plt.title('Comparison of Roughness Across Different Materials')
plt.tight_layout()
plt.show()

import seaborn as sns

plt.figure(figsize=(10, 8))
correlation_matrix = data.corr()
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm")
plt.title('Correlation Between Features and Roughness')
plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 6))
sns.histplot(y_test, kde=True, color='blue', label='Actual', bins=10)
sns.histplot(y_pred, kde=True, color='orange', label='Predicted', bins=10)
plt.xlabel('Roughness')
plt.ylabel('Frequency')
plt.title('Distribution of Predicted vs Actual Roughness')
plt.legend()
plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(data.pivot_table(index='layer_height', columns='infill_density', values='roughness'), annot=True, fmt=".1f", cmap="coolwarm")
plt.title('Roughness Across Layer Height and Infill Density')
plt.xlabel('Infill Density')
plt.ylabel('Layer Height')
plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 6))
sns.histplot(residuals, kde=True, color='orange', bins=15)
plt.xlabel('Residuals (Actual - Predicted)')
plt.ylabel('Frequency')
plt.title('Distribution of Prediction Errors')
plt.tight_layout()
plt.show()

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

# Calculate metrics
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

# Create the table
metric_table = {
    "Metric": ["Mean Squared Error (MSE)", "Mean Absolute Error (MAE)", "Root Mean Squared Error (RMSE)", "RÂ² Score"],
    "Description": [
        "Average squared difference between predicted and actual values",
        "Average absolute difference between predicted and actual values",
        "Square root of MSE",
        "Proportion of variance explained by the model"
    ],
    "Value": [mse, mae, rmse, r2]
}

import pandas as pd
metric_df = pd.DataFrame(metric_table)
print(metric_df)

plt.figure(figsize=(10, 6))
plt.barh(features, feature_importances, color='skyblue')
plt.xlabel('Feature Importance')
plt.ylabel('Features')
plt.title('Feature Importance for Predicting Roughness')
plt.tight_layout()
plt.show()

!pip install ultralytics

import zipfile
import os

# Define paths
zip_path = "/content/3Dprinting.v6-augmented.yolov8.zip"
extract_path = "/content/3Dprinting_dataset"

# Unzip the file
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)
print(f"Dataset extracted to: {extract_path}")

!ls /content/3Dprinting_dataset

from ultralytics import YOLO

results = model.val()
print(results)

from ultralytics import YOLO

# Load YOLOv8 Nano model
model = YOLO('yolov8n.pt')  # YOLOv8 Nano is lightweight and faster

# Train the model with optimized parameters
model.train(
    data="/content/3Dprinting_dataset/data.yaml",  # Path to dataset YAML file
    epochs=10,  # Reduced number of epochs for faster training
    imgsz=416,  # Smaller image size to speed up processing
    batch=8,    # Smaller batch size to optimize memory and speed
    name="3Dprinting_defect_detection_fast"  # Unique name for this run
)

model = YOLO('yolov8n.pt')  # Use the pre-trained YOLOv8 nano model (you can choose other sizes)

# Train the model
model.train(
    data="/content/3Dprinting_dataset/data.yaml",  # Path to dataset YAML file
    epochs=50,  # Number of training epochs
    imgsz=640,  # Image size
    batch=16,   # Batch size
    name="3Dprinting_defect_detection"
)

